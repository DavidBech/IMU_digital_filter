{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angles (3) + Accel (3) + Compass(3)\n",
    "#TODO all data input to model\n",
    "#TODO both IMUS\n",
    "\n",
    "#Quantity of indexes in input vector width \n",
    "data_input_width = 3 \n",
    "\n",
    "#Data is sent in with this many samples of width\n",
    "time_steps= 10\n",
    "\n",
    "#Minimum degree value required for an overcompensation measurement\n",
    "minimum_degree_for_overcomp = 15\n",
    "\n",
    "#Change this bit to True To train the model otherwise will load from the saved model\n",
    "train_model = False\n",
    "\n",
    "#Model Name is the name of the directory to load the model or save a new model to \n",
    "model_name = \"temp_model\"\n",
    "\n",
    "#Number of times the input data should be trained on \n",
    "epochs = 5\n",
    "\n",
    "#TODO for ensc 440\n",
    "# Change output of model to:\n",
    "#   [Abduction Confidence, Flexion Confidence, External Rotation Confidence, Unknown Exercise Confidence, Overcompensation Confidence of identified exercise]\n",
    "#   ie. The model will make a prediction of what exercise is being performed as well as if that exercise is being performed correctly\n",
    "#\n",
    "# Collect data for other exercises and for wrong forms of all exercises\n",
    "# Detect Start and end of exercise in waveform with multiple exercises, at least run the model on this and see if the output is reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_for_training():\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Simple RNN Network takes vector of time_steps elements of which each element is data_input_width wide\n",
    "    model.add(layers.SimpleRNN(32, input_shape = (time_steps, data_input_width), return_sequences=True))\n",
    "    model.add(layers.LSTM(16))\n",
    "    model.add(layers.Dense(10))\n",
    "\n",
    "    #sigmoid has output between 0 and 1 and represents the confidence of the prediction\n",
    "    model.add(layers.Dense(1, activation='sigmoid')) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToMatrix(data, step):\n",
    "    X = []\n",
    "    for i in range(len(data) - step):\n",
    "        d = i+step\n",
    "        X.append(data[i:d,])\n",
    "    return np.array(X)\n",
    "\n",
    "def read_format_csv(filename):\n",
    "    #TODO does the model perform better with this data?\n",
    "    #       If so do we need all of the data or just some?\n",
    "    ignore_cols = (\"time\",\"IMU#\",\"accelx\",\"accely\",\"accelz\",\"compassx\",\"compassy\",\"compassz\")\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    for col in ignore_cols:\n",
    "        data.pop(col)\n",
    "\n",
    "    return np.array(data)\n",
    "\n",
    "def get_in(filename):\n",
    "    inputs = convertToMatrix(read_format_csv(filename), time_steps)\n",
    "    return inputs\n",
    "\n",
    "def get_expect(inputs, over_comp):\n",
    "    if over_comp:\n",
    "        outputs = np.ones(len(inputs))\n",
    "        for i, vec in enumerate(inputs):\n",
    "            outputs[i] = 1 if abs(max(vec[0], key=abs)) > minimum_degree_for_overcomp else 0\n",
    "    else:\n",
    "        outputs = np.zeros(len(inputs))\n",
    "    return outputs\n",
    "\n",
    "#inputs = get_in(\"./data/abduction_validate/overcomp/imu0/imu0_21_9__15.csv\")\n",
    "#outputs = get_expect(inputs, 1)\n",
    "#print(inputs)\n",
    "#print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_file_names_imu0 = glob(\"./data/abduction_train/overcomp/imu0/*\")\n",
    "training_file_names_imu0.extend(glob(\"./data/abduction_train/reg/imu0/*\"))\n",
    "\n",
    "validation_file_names_imu0 = glob(\"./data/abduction_validate/overcomp/imu0/*\")\n",
    "validation_file_names_imu0.extend(glob(\"./data/abduction_validate/reg/imu0/*\"))\n",
    "\n",
    "files_per_epoch = len(training_file_names_imu0)\n",
    "files_per_val = len(validation_file_names_imu0)\n",
    "\n",
    "training_file_names_imu1 = [i.replace(\"imu0\", \"imu1\") for i in training_file_names_imu0]\n",
    "validation_file_names_imu1 = [i.replace(\"imu0\", \"imu1\") for i in validation_file_names_imu0]\n",
    "\n",
    "training_index = list(range(0, files_per_epoch))\n",
    "validation_index = list(range(0, files_per_val))\n",
    "\n",
    "def training_generator():\n",
    "    for j in range(epochs):\n",
    "        random.shuffle(training_index)\n",
    "        for i in range(files_per_epoch):\n",
    "            #TODO Input IMU1 Data to model?\n",
    "            file_name_imu0 = training_file_names_imu0[training_index[i]]\n",
    "            file_name_imu1 = training_file_names_imu1[training_index[i]]\n",
    "            temp_in = get_in(file_name_imu0)\n",
    "            temp_out = get_expect(temp_in, \"overcomp\" in file_name_imu0)\n",
    "            yield (temp_in, temp_out)\n",
    "\n",
    "def validation_generator():\n",
    "     for j in range(epochs):\n",
    "        random.shuffle(validation_index)\n",
    "        for i in range(files_per_val):\n",
    "            file_name_imu0 = validation_file_names_imu0[validation_index[i]]\n",
    "            file_name_imu1 = validation_file_names_imu1[validation_index[i]]\n",
    "            temp_in = get_in(file_name_imu0)\n",
    "            temp_out = get_expect(temp_in, \"overcomp\" in file_name_imu0)\n",
    "            yield (temp_in, temp_out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line is to get rid of a warning\n",
    "tf.config.experimental_functions_run_eagerly = False\n",
    "if train_model:\n",
    "    model = get_model_for_training()\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        x=training_generator(), \n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=files_per_epoch,\n",
    "        validation_data=validation_generator(),\n",
    "        validation_steps=files_per_val,\n",
    "        validation_freq=2\n",
    "    )\n",
    "\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = keras.models.load_model(model_name)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_with_predict(filename):\n",
    "    input_data = read_format_csv(filename)\n",
    "    # data is [[a0(0), a1(0), a2(0)], [a0(1), a1(1), a2(1)], ... , [a0(n-1), a1(n-1), a2(n-1)]]\n",
    "    \n",
    "    input_data_flat = input_data.flatten()\n",
    "    # flattened data is [a0(0), a1(0), a2(0), a0(1), a1(1), a2(1), ... , a0(n-1), a1(n-1), a2(n-1)]\n",
    "\n",
    "    # The following lines makes 3 vectors taking the 3rd entries of the flattened input\n",
    "    angle0 = input_data_flat[::3]\n",
    "    angle1 = input_data_flat[1::3]\n",
    "    angle2 = input_data_flat[2::3]\n",
    "\n",
    "    data_in = get_in(filename) \n",
    "    prediction = model.predict(data_in) # Predict the output for the input data\n",
    "\n",
    "    # Gives vector of 0 to len(angle0) spaced by 1\n",
    "    x_points = np.arange(0,len(angle0))\n",
    "\n",
    "    prediction_comp = prediction.flatten()\n",
    "\n",
    "    # data is fed in 10 points at a time so need to add 10 zero points \n",
    "    #   at start of prediction and angle vectors the same length\n",
    "    for i in range(time_steps):\n",
    "        prediction_comp = np.insert(prediction_comp, 0, 0)\n",
    "\n",
    "    # Scale prediction from (0 to 1) to (0 to 100)\n",
    "    prediction_comp *= 100\n",
    "\n",
    "    plt.plot(x_points, angle0, label=\"A0\")\n",
    "    plt.plot(x_points, angle1, label=\"A1\")\n",
    "    plt.plot(x_points, angle2, label=\"A2\")\n",
    "    plt.plot(x_points, prediction_comp, label=\"Over-C\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#file_name = \"./data/abduction_validate/reg/imu0/imu0_18_17__15.csv\"\n",
    "#file_name = \"./data/abduction_validate/reg/imu0/imu0_13_32__24.csv\"\n",
    "#plot_data_with_predict(file_name)\n",
    "\n",
    "print(\"Start of Overcompensation Runs\")\n",
    "for file in glob(\"./data/abduction_validate/overcomp/imu0/*\"):\n",
    "    plot_data_with_predict(file)\n",
    "\n",
    "print(\"Start of Regular Runs\")\n",
    "for file in glob(\"./data/abduction_validate/reg/imu0/*\"):\n",
    "    plot_data_with_predict(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
